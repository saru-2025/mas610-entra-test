{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f640126-d0e2-49e4-8edd-117b9451de58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook: MAS610_Gold_FactExposure\n",
    "# Purpose: join & derive MAS610 exposure metrics (Exercise 2)\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MAS610_Gold_FactExposure\").getOrCreate()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Parameters\n",
    "# ---------------------------------------------------------------------\n",
    "dbutils.widgets.text(\"silver_dir\", \"/mnt/silver/mas610_ex2\")\n",
    "dbutils.widgets.text(\"gold_dir\", \"/mnt/gold/mas610\")\n",
    "silver_dir = dbutils.widgets.get(\"silver_dir\")\n",
    "gold_dir   = dbutils.widgets.get(\"gold_dir\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Read Silver tables\n",
    "# ---------------------------------------------------------------------\n",
    "accounts = spark.read.format(\"delta\").load(f\"{silver_dir}/accounts_silver.delta\")\n",
    "loans    = spark.read.format(\"delta\").load(f\"{silver_dir}/loans_silver.delta\")\n",
    "collat   = spark.read.format(\"delta\").load(f\"{silver_dir}/collateral_silver.delta\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Join & Enrich ‚Üí MAS610 Fact Exposure\n",
    "# ---------------------------------------------------------------------\n",
    "fact_exposure = (\n",
    "    loans\n",
    "      .join(accounts, \"customer_id\", \"left\")\n",
    "      .join(collat, \"loan_id\", \"left\")\n",
    "      .withColumn(\"risk_weight\",\n",
    "          F.when(F.col(\"collateral_value\").isNull() | (F.col(\"collateral_value\")==0), 100.0)\n",
    "           .otherwise(50.0))\n",
    "      .withColumn(\"EAD\", F.col(\"notional\"))\n",
    "      .withColumn(\"RWA\", F.round(F.col(\"EAD\") * F.col(\"risk_weight\") / 100, 2))\n",
    "      .withColumn(\"secured_flag\", F.when(F.col(\"collateral_value\") > 0, 1).otherwise(0))\n",
    "      .withColumn(\"reporting_date\", F.lit(datetime.today().strftime(\"%Y-%m-%d\")))\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Remove duplicate columns before writing\n",
    "# ---------------------------------------------------------------------\n",
    "fact_exposure = fact_exposure.drop(\n",
    "    *[c for c in fact_exposure.columns if c.startswith(\"load_timestamp\")]\n",
    ")\n",
    "fact_exposure = fact_exposure.withColumn(\"load_timestamp\", F.current_timestamp())\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Basic DQ validation (assertion)\n",
    "# ---------------------------------------------------------------------\n",
    "null_customers = fact_exposure.filter(F.col(\"customer_id\").isNull()).count()\n",
    "bad_weights    = fact_exposure.filter(~F.col(\"risk_weight\").between(0,150)).count()\n",
    "assert null_customers==0 and bad_weights==0, \"‚ùå DQ validation failed!\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Persist Gold outputs\n",
    "# ---------------------------------------------------------------------\n",
    "fact_exposure.write.mode(\"overwrite\").format(\"delta\").save(f\"{gold_dir}/fact_exposure.delta\")\n",
    "fact_exposure.write.mode(\"overwrite\").json(f\"{gold_dir}/json\")\n",
    "fact_exposure.write.mode(\"overwrite\").parquet(f\"{gold_dir}/parquet\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# MAS610 Report View\n",
    "# ---------------------------------------------------------------------\n",
    "fact_exposure.createOrReplaceTempView(\"fact_exposure\")\n",
    "mas610_report = spark.sql(\"\"\"\n",
    "SELECT customer_id AS Customer_ID, loan_id AS Loan_ID, account_type AS Account_Type,\n",
    "       branch_code AS Branch_Code, region AS Region, currency AS Currency,\n",
    "       notional AS Exposure_Amount, collateral_value AS Collateral_Value,\n",
    "       risk_weight AS Risk_Weight_Pct, EAD AS Exposure_at_Default, RWA AS Risk_Weighted_Asset,\n",
    "       secured_flag AS Secured_Flag, reporting_date AS Reporting_Date\n",
    "FROM fact_exposure ORDER BY Customer_ID, Loan_ID\n",
    "\"\"\")\n",
    "mas610_report.write.mode(\"overwrite\").parquet(f\"{gold_dir}/MAS610_Report.parquet\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Optional Summary\n",
    "# ---------------------------------------------------------------------\n",
    "summary = (\n",
    "    mas610_report.groupBy(\"Currency\")\n",
    "      .agg(F.countDistinct(\"Loan_ID\").alias(\"Loan_Count\"),\n",
    "           F.sum(\"Exposure_at_Default\").alias(\"Total_EAD\"),\n",
    "           F.sum(\"Risk_Weighted_Asset\").alias(\"Total_RWA\"),\n",
    "           F.round(F.sum(\"Risk_Weighted_Asset\")/F.sum(\"Exposure_at_Default\")*100,2).alias(\"Avg_Risk_Pct\"))\n",
    ")\n",
    "summary.write.mode(\"overwrite\").csv(f\"{gold_dir}/summary_currency.csv\", header=True)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# ‚úÖ Extended DQ Results Logging (Optional but recommended)\n",
    "# ---------------------------------------------------------------------\n",
    "dq_rules = []\n",
    "dq_rules.append((\"DQ_01\", \"Null_CustomerID\",\n",
    "                 fact_exposure.filter(F.col(\"customer_id\").isNull()).count()))\n",
    "dq_rules.append((\"DQ_02\", \"Invalid_RiskWeight\",\n",
    "                 fact_exposure.filter(~F.col(\"risk_weight\").between(0,150)).count()))\n",
    "dq_rules.append((\"DQ_03\", \"Negative_Notional\",\n",
    "                 fact_exposure.filter(F.col(\"notional\") < 0).count()))\n",
    "\n",
    "dq_df = spark.createDataFrame(dq_rules, [\"Rule_ID\", \"Rule_Name\", \"Failed_Count\"]) \\\n",
    "             .withColumn(\"Total_Records\", F.lit(fact_exposure.count())) \\\n",
    "             .withColumn(\"Pass_Flag\", F.when(F.col(\"Failed_Count\")==0,\"PASS\").otherwise(\"FAIL\")) \\\n",
    "             .withColumn(\"DQ_Run_TS\", F.current_timestamp())\n",
    "\n",
    "dq_df.write.mode(\"overwrite\").format(\"delta\").save(f\"{gold_dir}/dq_results.delta\")\n",
    "\n",
    "print(\"üíæ DQ results written to dq_results.delta\")\n",
    "display(dq_df)\n",
    "\n",
    "print(\"üèÅ Gold layer build & MAS610 calculation complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MAS610_Gold_FactExposure",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
